{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the condition of water wells in Tanzania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/water_fill.jpg\" \n",
    "     align=\"left\" \n",
    "     width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "test_set_values = pd.read_csv('Data/test_set_values.csv')\n",
    "training_set_labels = pd.read_csv('Data/training_set_labels.csv')\n",
    "training_set_values = pd.read_csv('Data/training_set_values.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging training set values and training set labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ac7dea0c1f35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#inner merge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtraining_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_set_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#inner merge\n",
    "training_all = pd.merge(training_set_values, training_set_labels, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-df70978d906b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'training_all' is not defined"
     ]
    }
   ],
   "source": [
    "training_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning - dealing with N/A values and removing unnecessary data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the num_private column because there is no data on what this column means on the website source of the data\n",
    "training_all.drop(columns='num_private', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace specific values in the 'installer' column\n",
    "training_all['installer'] = training_all['installer'].replace('Hesawa', 'HESAWA')\n",
    "training_all['installer'] = training_all['installer'].replace('DANID', 'DANIDA')\n",
    "training_all['installer'] = training_all['installer'].replace('Commu', \"Community\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the name of some the columns so they are more intuitive\n",
    "training_all.rename(columns={'gps_height': 'well_altitude'}, inplace=True)\n",
    "training_all.rename(columns={'wpt_name': 'water_point_name'}, inplace=True)\n",
    "training_all.rename(columns={'payment': 'payment_type'}, inplace=True)\n",
    "training_all.rename(columns={'payment_type': 'frequency_of_payment'}, inplace=True)\n",
    "training_all.rename(columns={'funder': 'funding_source'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing three options for functioning status of wells to two options - functional or needing repair\n",
    "to_replace = ['functional needs repair']\n",
    "new_value = 'functional'\n",
    "training_all['status_group'] = training_all['status_group'].replace(to_replace, new_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all.dropna(subset=['latitude'], inplace=True)\n",
    "training_all.dropna(subset=['longitude'], inplace=True)\n",
    "training_all = training_all.loc[training_all['longitude'] != 0]\n",
    "training_all = training_all.loc[training_all['latitude'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Data Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with construction year of 0\n",
    "training_all.drop(training_all[training_all['construction_year'] == 0].index, inplace=True)\n",
    "training_all['construction_year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For installer, filter out those with less than 500 count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate installer counts\n",
    "installer_counts = training_all['installer'].value_counts()\n",
    "\n",
    "# Identify installers with counts >= 500\n",
    "installers_to_keep = installer_counts[installer_counts >= 500].index\n",
    "\n",
    "# Filter the DataFrame based on installers to keep\n",
    "training_all = training_all[training_all['installer'].isin(installers_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheme management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop n/a\n",
    "training_all.dropna(subset=['scheme_management'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"None\" and \"Other\" values\n",
    "training_all = training_all.loc[training_all['scheme_management'] != 'None']\n",
    "training_all = training_all.loc[training_all['scheme_management'] != 'Other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all['extraction_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwhelming majority of pumps have gravity as extraction type, so maybe not worth it to filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction type class makes more sense, seems more organized into extraction types that could be compared, \n",
    "# but still most pumps use gravity extraction method\n",
    "training_all['extraction_type_class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Management or management group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all['management'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all['management_group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all['quantity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all['quantity_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop quantity_group column because it is a duplicate of quantity column \n",
    "training_all.drop(columns='quantity_group', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unknown values from quantity column\n",
    "training_all = training_all.loc[training_all['quantity'] != 'unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unknown values from quantity column\n",
    "training_all = training_all.loc[training_all['source'] != 'unknown']\n",
    "training_all = training_all.loc[training_all['source'] != 'other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waterpoint type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all['waterpoint_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all['waterpoint_type_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all['status_group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'installer' with the actual column name\n",
    "installer_counts = training_all['installer'].value_counts()\n",
    "\n",
    "# Get installer types with less than 2 instances\n",
    "installers_to_remove = installer_counts[installer_counts < 500].index\n",
    "\n",
    "# Filter the dataset to exclude rows with those installer types\n",
    "installer_filtered = training_all[~training_all['installer'].isin(installers_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installer_filtered = installer_filtered.loc[installer_filtered['installer'] != '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installer_filtered['installer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installer_dummy = pd.get_dummies(installer_filtered, columns=['installer'], prefix='installer')\n",
    "installer_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = installer_dummy[['installer_CES', 'installer_Central government', 'installer_Community', 'installer_DANIDA', 'installer_DWE', 'installer_District Council', 'installer_Government', 'installer_HESAWA', 'installer_KKKT', 'installer_RWE', 'installer_TCRS']]  # Drop the target column\n",
    "y = installer_dummy['status_group']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = training_all[['status_group','construction_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year.drop(year[year['construction_year'] == 0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year['construction_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year['construction_year'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'data' and the column to convert is 'status_group'\n",
    "year['status_group'] = year['status_group'].map({'functional': 1, 'non functional': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming you have a DataFrame named 'data' with columns 'year' and 'status_group'\n",
    "# 'status_group' should be encoded as 0 (not functional) and 1 (functional)\n",
    "\n",
    "# Create a constant term to include in the model\n",
    "year['constant'] = 1\n",
    "\n",
    "# Define the independent variable (X) and dependent variable (y)\n",
    "X = year[['constant', 'construction_year']]\n",
    "y = year['status_group']\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = sm.Logit(y, X).fit()\n",
    "\n",
    "# Print the summary of the regression\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = model.predict(X)\n",
    "residuals = y - predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X['construction_year'], residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Add a horizontal line at y=0\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a range of years for prediction\n",
    "years_for_prediction = np.arange(year['construction_year'].min(), year['construction_year'].max() + 1)\n",
    "\n",
    "# Create a new DataFrame for prediction\n",
    "prediction_data = pd.DataFrame({'constant': 1, '_construction_year': years_for_prediction})\n",
    "\n",
    "# Calculate predicted probabilities for each year\n",
    "predicted_probs = model.predict(prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(years_for_prediction, predicted_probs, marker='o')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Predicted Probability (status_group=0)')\n",
    "plt.title('Predicted Probability of Non-Functional Water Point')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi squared test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "alpha = 0.05\n",
    "# Calculate the degrees of freedom for the Chi-squared test\n",
    "degrees_of_freedom = (contingency_table.shape[0] - 1) * (contingency_table.shape[1] - 1)\n",
    "\n",
    "critical_value = chi2.ppf(1 - alpha, degrees_of_freedom)\n",
    "# List of categorical variables to test\n",
    "categorical_vars = ['waterpoint_type', 'installer', 'scheme_management', 'construction_year', 'extraction_type', 'management', 'quantity', 'source']\n",
    "\n",
    "for var in categorical_vars:\n",
    "    contingency_table = pd.crosstab(training_all[var], training_all['status_group'])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    if chi2 > critical_value:\n",
    "        print(f\"{var} and target are dependent\")\n",
    "    else:\n",
    "        print(f\"{var} and target are independent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
